environment:
  start_node: auto
  max_steps: auto
  max_wait_steps: auto
  render_mode: "human"
  shortest_path_algorithm: "astar"

rewards:
  weight_name: "travel_time"
  anti_loop_penalty: 20.0
  move_cost_coef: 0.01
  progress_coef: 5.0
  waypoint_bonus: 50.0
  destination_bonus: 200.0
  no_progress_penalty: 2.0


graph:
  source: "place"        # "place" | "file"
  place: "Río Cuarto, Cordoba, Argentina"
  path: ""               # ruta a .graphml si source == "file"
  distances_path: ""      # ruta a .pkl opcional con distancias precomputadas

# hiperparámetros de PPO y entrenamiento
ppo:
  policy: "MlpPolicy"
  device: "cuda"  # "auto" | "cuda" | "cpu"
  learning_rate: 3e-4
  n_steps: 1024
  batch_size: 256
  gamma: 0.99
  clip_range: 0.2
  ent_coef: 0.01
  vf_coef: 0.5
  net_arch: [256, 128, 64]
  total_timesteps: 250000
  verbose: 1

evaluation:
  eval_freq: 5000
  n_eval_episodes: 5
  deterministic: true
  best_model_save_path: "./logs/best_model_masked/"
  log_path: "./logs/results_masked/"
  train_log_dir: "./logs/training/"  # directorio donde se guarda monitor.csv con recompensas de entrenamiento
  tensorboard_log: "./logs/tensorboard/"  # directorio donde TensorBoard guarda métricas de convergencia (losses, entropy, etc.)
  early_stop:
    max_no_improvement_evals: 8
    min_evals: 3
    verbose: 1
